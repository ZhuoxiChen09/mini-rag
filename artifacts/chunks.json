[
  {
    "doc": "sample_1.txt",
    "chunk_id": 0,
    "text": "Large language models (LLMs) are machine learning systems trained to predict the next token in a sequence of text. Although the training objective is simple, when models are scaled up in data, parameters, and compute, they begin to exhibit surprisingly rich behaviors. They can answer questions, summarize documents, translate between languages, and follow natural language instructions. Modern LLMs are typically based on the Transformer architecture, which uses self-attention mechanisms to process sequences in parallel. During training, the model is exposed to massive text corpora, such as web pages, books, and code. The model learns statistical patterns of language, but these patterns often encode useful world knowledge and reasoning heuristics. LLMs do not “understand” language in a human sense, but they can approximate understanding by leveraging the correlations they have learned. Because they are trained only to predict tokens, they can sometimes hallucinate or produce confident but incorrect statements. For this reason, LLM outputs should be checked when used in high-stakes settings. A growing area of research focuses on aligning LLM behavior with human intents and values. Techniques such as instruction tuning and reinforcement learning from human feedback (RLHF) can make models more helpful, honest, and harmless. Another practical direction is retrieval-augmented generation, where an LLM is combined with a search or retrieval system so that it can ground its answers in external data."
  },
  {
    "doc": "sample_2.txt",
    "chunk_id": 0,
    "text": "The Transformer architecture was introduced in the paper “Attention Is All You Need” in 2017. Unlike recurrent neural networks, Transformers process all tokens in a sequence simultaneously using a mechanism called self-attention. This allows them to model long-range dependencies more effectively and to take advantage of parallel computation on modern hardware. A Transformer encoder layer typically consists of multi-head self-attention and a position-wise feed-forward network, both wrapped with residual connections and layer normalization. Self-attention lets each token attend to every other token in the sequence, computing weighted combinations of token representations. Multi-head attention repeats this process several times in parallel, allowing the model to capture different types of relationships. Position information is injected using positional encodings or learned position embeddings, because the self-attention mechanism itself is permutation-invariant. By stacking many layers of attention and feed-forward blocks, Transformers can build hierarchical representations of text, images, or other modalities. Transformers have become the dominant architecture in natural language processing and are also widely used in vision, speech, and multimodal models. Large language models such as GPT, PaLM, and LLaMA are all based on Transformer decoders. Their ability to scale with more parameters and data is a key reason for the recent progress in generative AI."
  }
]